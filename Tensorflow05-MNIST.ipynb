{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类算法：以MNIST为例\n",
    "\n",
    "B站地址：https://www.bilibili.com/video/av43395877/\n",
    "\n",
    "之前我们做了数值拟合的程序，他们得到的结果仅仅是一个数，得到的误差函数也仅仅是使用了误差平方和的形式。\n",
    "\n",
    "现在考虑分类问题，分类问题最重要的是把正确的类别识别出来，得到的结果并不是完全是像拟合任务一样，label值并不是单纯的可以比较大小的数值，所以上述误差函数并不是特别适用。\n",
    "\n",
    "### 结果的表示\n",
    "\n",
    "为了实现分类的效果，我们需要用一种形式来表明结果代表的含义。\n",
    "\n",
    "之前在拟合异或的时候，曾经提到过，异或的结果0或者1可以被认为是一种分类，所以二分类可以简单使用0到1的值（通过sigmoid函数即可）。但是，如果多分类采用0，1，2，3等数值来表示，相当于给定了一个大小关系与排列顺序，这是不科学的。\n",
    "\n",
    "#### one-hot编码&softmax\n",
    "\n",
    "一共有N种可能的分类，则可以使用一个长度为N的数组，每个元素均为0-1的数，总和为1。\n",
    "\n",
    "softmax函数用来归一化，即把所有的数据变到0-1的区间，同时保证其和为1.\n",
    "\n",
    "- 为了能够变为0或者正数，将每一个数据都用指数函数处理；\n",
    "- 为了能够保证和为1，把所有元素都除以总和，加起来就是1.\n",
    "\n",
    "函数形式为：\n",
    "\n",
    "- $\\frac{e^{a_m}}{\\sum\\limits_{i=1}^{n} e^{a_i}}$\n",
    "\n",
    "计算的时候，为了避免指数计算得到全是0或者极大值，可以让所有的$a_i$全都增加或减小一个数$M$（相当于分子分母都同时乘上$e^M$）。比如直接取$-\\max \\{a_i\\}$\n",
    "\n",
    "具体的编码方式：\n",
    "- 表示标签数据时，可以直接将它代表的标签那一位置为1，其余置为0，即所谓one-hot编码；\n",
    "- 神经网络的输出设置为长度为N的数组，最后计算得到的结果，使用softmax函数确定。\n",
    "\n",
    "### loss函数的确定\n",
    "\n",
    "#### 信息熵\n",
    "\n",
    "变量的不确定性越大，熵也越大，即把它搞清楚所需要的信息量也越大。\n",
    "\n",
    "信息熵的公式是：$\\sum\\limits_{i=1}^n p_i \\ln \\frac 1 {p_i} = -\\sum\\limits_{i=1}^n p_i \\ln {p_i}$\n",
    "\n",
    "*注：ln代表以2为底*\n",
    "\n",
    "比如网上看到的一个编码问题：\n",
    "\n",
    "> 如果有A、B、C、D这四个字符组成的内容，每个字符出现的概率都是1/4，即概率分布为{1/4，1/4，1/4，1/4}，设计一个最短的编码方案来表示一组数据，套用刚才的公式：\n",
    "> \n",
    "> 算出来信息熵为2，即需要2bit来表示。如果第1位0表示A，1表示B；第2位0表示C，1表示D，2位编码搞定。\n",
    "> \n",
    "> 如果概率变了，比如A、B、C、D出现的概率是{0，0，1/2，1/2}，即：每次A、B必然不出现，C、D出现机会各占一半，这样只要1位就可以了。1表示C，0表示D，因为AB必然不出现，不用表示都知道肯定不用AB，套用公式算出来的结果也是如此（注意0ln0需要使用极限来算，得到0）。\n",
    "\n",
    "#### 交叉熵\n",
    "\n",
    "$H(x,y)=-\\sum\\limits_{i=1}^n x_i\\ln y_i$，这里x是已知的概率分布，y是自己算出来的概率分布，结果越小表明越接近。\n",
    "\n",
    "本质上来说，它来源于极大似然函数。考虑$p(t|x) = \\prod_{i=1}^{C}P(t_i|x)^{t_i} = \\prod_{i=1}^{C}y_i^{t_i}$，其中$t$代表我们实际得到的标签，$y$或者$P(t_i |x)$指计算得到的结果，取对数即为上述交叉熵。\n",
    "\n",
    "交叉熵得到的是单个样本的误差，再次求平均得到的就是这一批次样本的总的误差。\n",
    "\n",
    "之前的训练过程中，因为所有样本全部参与训练，所以求平均一步也写成了求和；但是对于样本不一定个数相同的情况，还是写成求平均（reduce_mean）。\n",
    "\n",
    "##### 交叉熵在tensorflow中的实现\n",
    "\n",
    "我们在softmax之后，可以直接用`tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))`来得到所有样本交叉熵的平均值。\n",
    "\n",
    "其中reduction_indices表示在第几个维度上进行reduce，这里是首先对于一个样本中的元素进行reduce，得到的就是一个样本的交叉熵。\n",
    "\n",
    "![reduction_indices](Tensorflow05-MNIST.assets/v2-c92ac5c3a50e4bd3d60e29c2ddc4c5e9_r.png)\n",
    "\n",
    "但是为了方便起见，我们可以直接使用`tf.nn.softmax_cross_entropy_with_logits`来进行softmax和交叉熵计算的操作。\n",
    "\n",
    "参考：\n",
    "\n",
    "- https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5\n",
    "- https://www.imooc.com/article/30013\n",
    "- https://zhuanlan.zhihu.com/p/27223959\n",
    "- http://www.cnblogs.com/likethanlove/p/6547405.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST网络构建\n",
    "\n",
    "### 数据读入\n",
    "\n",
    "现在还可以直接使用官方提供的mnist数据集及其相关函数，但是其中一些会很快被弃用：\n",
    "\n",
    "- 数据集使用：`from tensorflow.examples.tutorials.mnist import input_data`\n",
    "- 读取数据集中数据：`mnist = input_data.read_data_sets('MNIST_data', one_hot=True)`\n",
    "- 数据集中可以分批读入数据，使用`mnist.train.next_batch(100)`每次读入100个数据。\n",
    "\n",
    "MNIST数据集中的元素都是长度为$28*28=784$的数组，所以输入矩阵的大小为`[None, 784]`，label的大小为`[None, 10]`，这里为了简单，不设置更多层，直接在一个线性模型后面接上softmax来进行计算。\n",
    "\n",
    "*注：建议提前下载并且保存MNIST数据集到本地MNIST_data文件夹中，避免下载用时过长*\n",
    "\n",
    "具体代码如下，参考https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/5-01-classifier/ ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-036f4c261197>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\koala_fh_ssd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\koala_fh_ssd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\koala_fh_ssd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\koala_fh_ssd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\koala_fh_ssd\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.1037\n",
      "0.6313\n",
      "0.7415\n",
      "0.782\n",
      "0.8066\n",
      "0.8228\n",
      "0.8295\n",
      "0.8361\n",
      "0.8446\n",
      "0.8512\n",
      "0.8577\n",
      "0.8607\n",
      "0.866\n",
      "0.8643\n",
      "0.8669\n",
      "0.8727\n",
      "0.8698\n",
      "0.8729\n",
      "0.8743\n",
      "0.8778\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys})\n",
    "    return result\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None, 28 * 28])\n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "prediction = add_layer(xs, 28 * 28, 10, activation_function=tf.nn.softmax)\n",
    "\n",
    "# 交叉熵作为loss函数，内层计算得到一个样本的loss，之后平均得到整体的样本loss\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys})\n",
    "        if i % 50 == 0:\n",
    "            print(compute_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果需要自己手动加载数据集，则可以使用如下方法（来自https://blog.csdn.net/simple_the_best/article/details/75267863）：\n",
    "\n",
    "首先解压下载好的四个MNIST数据压缩包，放入同一个文件夹中（比如我放到了`MNIST_data`文件夹中）。\n",
    "\n",
    "之后使用大端序进行读取，读完前面的magic number和个数信息后，将剩余的数据作为numpy数组进行读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "4\n",
      "1\n",
      "9\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels.idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images.idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "images, labels = load_mnist('MNIST_data')\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=5,\n",
    "    sharex=True,\n",
    "    sharey=True) # share坐标轴以便于后续去除坐标轴\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "    print(labels[i])\n",
    "\n",
    "# 去掉坐标轴\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
